# Workflow Engine â€“ Integration Test Szenario

> **Zweck:** Beschreibt ein vollstÃ¤ndiges End-to-End Szenario fÃ¼r die WorkflowEngine.
> Kann als Basis fÃ¼r zukÃ¼nftige E2E Tests oder manuelle QA dienen.
>
> **Status:** Dokumentiert â€“ Unit Tests sind in `workflow.test.ts` implementiert.
> Dieses File beschreibt, wie ein echter E2E Test aussehen wÃ¼rde.

---

## Szenario: "Analysiere eine Python-Datei und erstelle eine Zusammenfassung"

### User Story
Ein User gibt folgende Aufgabe: *"Lies die Datei `analysis.py`, analysiere den Code und speichere eine Zusammenfassung als `summary.md`."*

### Erwarteter Ablauf

```
User: "Lies die Datei analysis.py, analysiere den Code und speichere eine Zusammenfassung als summary.md"
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 1: PLANNING                          â”‚
â”‚  LLM erstellt strukturierten JSON-Plan      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚  {                                          â”‚
â”‚    "goal": "Code analysieren + Doku",       â”‚
â”‚    "steps": [                               â”‚
â”‚      { "id": "step-1",                      â”‚
â”‚        "description": "Datei lesen",        â”‚
â”‚        "expectedTools": ["read_file"] },    â”‚
â”‚      { "id": "step-2",                      â”‚
â”‚        "description": "Code analysieren",   â”‚
â”‚        "expectedTools": [] },               â”‚
â”‚      { "id": "step-3",                      â”‚
â”‚        "description": "Doku schreiben",     â”‚
â”‚        "expectedTools": ["write_file"] }    â”‚
â”‚    ],                                       â”‚
â”‚    "maxSteps": 3                            â”‚
â”‚  }                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼ EVENT: plan
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 2: EXECUTING â€“ Step 1               â”‚
â”‚  executeAgentLoop() â†’ read_file("analysis.py")
â”‚  Ergebnis: "def fibonacci(n): ..."         â”‚
â”‚                                             â”‚
â”‚  EVENT: step_start (stepId: "step-1")      â”‚
â”‚  EVENT: tool_call  (name: "read_file")     â”‚
â”‚  EVENT: tool_result (success: true)        â”‚
â”‚  EVENT: step_end (status: "success")       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 3: REFLECTING â€“ Step 1               â”‚
â”‚  LLM bewertet: {"assessment": "success",   â”‚
â”‚                 "nextAction": "continue"}   â”‚
â”‚                                             â”‚
â”‚  EVENT: reflection (assessment: "success") â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 2: EXECUTING â€“ Step 2               â”‚
â”‚  executeAgentLoop() â†’ keine Tools           â”‚
â”‚  LLM analysiert Code intern                 â”‚
â”‚                                             â”‚
â”‚  EVENT: step_start (stepId: "step-2")      â”‚
â”‚  EVENT: step_end (status: "success")       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 3: REFLECTING â€“ Step 2               â”‚
â”‚  LLM: {"assessment": "success",            â”‚
â”‚         "nextAction": "continue"}           â”‚
â”‚                                             â”‚
â”‚  EVENT: reflection (assessment: "success") â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 2: EXECUTING â€“ Step 3               â”‚
â”‚  executeAgentLoop() â†’ write_file("summary.md", "...")
â”‚  Ergebnis: "File written successfully"     â”‚
â”‚                                             â”‚
â”‚  EVENT: step_start (stepId: "step-3")      â”‚
â”‚  EVENT: tool_call  (name: "write_file")    â”‚
â”‚  EVENT: tool_result (success: true)        â”‚
â”‚  EVENT: step_end (status: "success")       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 3: REFLECTING â€“ Step 3 (letzter)    â”‚
â”‚  LLM erkennt: Alle Steps erledigt          â”‚
â”‚  {"assessment": "success",                  â”‚
â”‚   "nextAction": "complete",                 â”‚
â”‚   "finalAnswer": "Die Datei wurde..."       â”‚
â”‚                                             â”‚
â”‚  EVENT: reflection (nextAction: "complete")â”‚
â”‚  EVENT: message (done: true)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 4: DONE                              â”‚
â”‚  STATUS: done                               â”‚
â”‚  finalAnswer gespeichert                    â”‚
â”‚                                             â”‚
â”‚  EVENT: workflow_end (status: "done")       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## VollstÃ¤ndige Event-Sequenz

| # | Event Type | Key Data |
|---|-----------|----------|
| 1 | `workflow_start` | workflowId, config |
| 2 | `plan` | 3 Steps, isAdjustment: false |
| 3 | `step_start` | stepId: "step-1", description: "Datei lesen" |
| 4 | `tool_call` | name: "read_file", args: { path: "analysis.py" } |
| 5 | `tool_result` | success: true, content: "def fibonacci..." |
| 6 | `step_end` | status: "success", durationMs: ~800 |
| 7 | `reflection` | assessment: "success", nextAction: "continue" |
| 8 | `step_start` | stepId: "step-2", description: "Code analysieren" |
| 9 | `step_end` | status: "success" |
| 10 | `reflection` | assessment: "success", nextAction: "continue" |
| 11 | `step_start` | stepId: "step-3", description: "Doku schreiben" |
| 12 | `tool_call` | name: "write_file", args: { path: "summary.md", content: "..." } |
| 13 | `tool_result` | success: true |
| 14 | `step_end` | status: "success" |
| 15 | `reflection` | assessment: "success", nextAction: "complete", finalAnswer: "..." |
| 16 | `message` | content: "Die Analyse ist abgeschlossen...", done: true |
| 17 | `workflow_end` | status: "done", totalSteps: 3, durationMs: ~5000 |

---

## BenÃ¶tigte Mocks fÃ¼r E2E Tests

### 1. Ollama API (`sendAgentChatMessage`)

```typescript
// Planning response
vi.mocked(sendAgentChatMessage).mockResolvedValueOnce({
  content: JSON.stringify({
    goal: "Python-Datei lesen, analysieren und dokumentieren",
    steps: [
      { id: "step-1", description: "Datei lesen", expectedTools: ["read_file"], dependsOn: [], successCriteria: "Inhalt verfÃ¼gbar" },
      { id: "step-2", description: "Code analysieren", expectedTools: [], dependsOn: ["step-1"], successCriteria: "Analyse vollstÃ¤ndig" },
      { id: "step-3", description: "Doku schreiben", expectedTools: ["write_file"], dependsOn: ["step-2"], successCriteria: "summary.md erstellt" },
    ],
    maxSteps: 3,
  }),
  tokenStats: null,
});

// Reflection after step-1
vi.mocked(sendAgentChatMessage).mockResolvedValueOnce({
  content: JSON.stringify({ assessment: "success", nextAction: "continue", comment: "Datei gelesen." }),
  tokenStats: null,
});

// Reflection after step-2
vi.mocked(sendAgentChatMessage).mockResolvedValueOnce({
  content: JSON.stringify({ assessment: "success", nextAction: "continue", comment: "Analyse fertig." }),
  tokenStats: null,
});

// Reflection after step-3 (early exit)
vi.mocked(sendAgentChatMessage).mockResolvedValueOnce({
  content: JSON.stringify({
    assessment: "success",
    nextAction: "complete",
    finalAnswer: "Die Python-Datei wurde analysiert und eine Zusammenfassung in summary.md gespeichert.",
  }),
  tokenStats: null,
});
```

### 2. Agent Executor (`executeAgentLoop`)

```typescript
// Step 1: read_file
vi.mocked(executeAgentLoop).mockImplementationOnce(async function*() {
  yield {
    index: 0,
    toolCalls: [{ id: "tc_001", name: "read_file", arguments: { path: "analysis.py" } }],
    toolResults: [{ callId: "tc_001", content: "def fibonacci(n):\n    ...", success: true }],
    startedAt: new Date().toISOString(),
    completedAt: new Date().toISOString(),
  };
});

// Step 2: no tools (analysis happens in LLM)
vi.mocked(executeAgentLoop).mockImplementationOnce(async function*() {
  yield {
    index: 0,
    toolCalls: [],
    toolResults: [],
    startedAt: new Date().toISOString(),
    completedAt: new Date().toISOString(),
    assistantMessage: "The code implements Fibonacci algorithm...",
  };
});

// Step 3: write_file
vi.mocked(executeAgentLoop).mockImplementationOnce(async function*() {
  yield {
    index: 0,
    toolCalls: [{ id: "tc_002", name: "write_file", arguments: { path: "summary.md", content: "# Analyse\n..." } }],
    toolResults: [{ callId: "tc_002", content: "File written successfully", success: true }],
    startedAt: new Date().toISOString(),
    completedAt: new Date().toISOString(),
  };
});
```

### 3. Filesystem (fÃ¼r echte Tool-AusfÃ¼hrung)

```typescript
// TemporÃ¤res Verzeichnis fÃ¼r den Test
const tmpDir = await fs.mkdtemp(path.join(os.tmpdir(), 'locai-e2e-'));
await fs.writeFile(path.join(tmpDir, 'analysis.py'), 'def fibonacci(n):\n    ...');
process.env.LOCAI_DATA_PATH = tmpDir;

// Nach dem Test aufrÃ¤umen
afterEach(async () => {
  await fs.rm(tmpDir, { recursive: true, force: true });
});
```

---

## Assertions fÃ¼r den E2E Test

```typescript
// Alle Events werden emittiert
expect(events.map(e => e.type)).toEqual([
  'workflow_start', 'plan',
  'step_start', 'tool_call', 'tool_result', 'step_end', 'reflection',
  'step_start', 'step_end', 'reflection',
  'step_start', 'tool_call', 'tool_result', 'step_end', 'reflection',
  'message',
  'workflow_end',
]);

// Workflow ist erfolgreich
expect(lastEvent.status).toBe('done');

// Finale Antwort enthÃ¤lt die richtigen Infos
const msgEvent = events.find(e => e.type === 'message');
expect(msgEvent.content).toContain('zusammenfassung');
expect(msgEvent.done).toBe(true);

// summary.md wurde erstellt
const summaryContent = await fs.readFile(path.join(tmpDir, 'summary.md'), 'utf-8');
expect(summaryContent).toBeTruthy();
```

---

## ZusÃ¤tzliche E2E Szenarien (fÃ¼r die Zukunft)

### Szenario 2: Plan-Adjustment
- Step schlÃ¤gt fehl (Tool-Fehler)
- Reflection erkennt: `adjust_plan`
- Neuer Plan mit alternativen Steps
- Test: 2 `plan` Events (initial + adjusted)

### Szenario 3: Timeout
- Workflow mit sehr kurzem `timeoutMs`
- Fake timers (`vi.useFakeTimers()`)
- Test: `workflow_end.status === 'timeout'`

### Szenario 4: Cancellation
- User bricht nach Step 1 ab
- Test: `cancelled` event + `workflow_end.status === 'cancelled'`

### Szenario 5: LLM-Fehler Recovery
- Planning LLM gibt kein valides JSON â†’ Fallback Plan
- Reflection wirft Exception â†’ Workflow geht weiter
- Test: Workflow endet mit `done` trotz Fehler

---

## Implementierungs-Hinweise

### Testdatei Location
```
src/lib/agents/workflow.e2e.test.ts  (noch zu erstellen)
```

### BenÃ¶tigte Imports
```typescript
import { WorkflowEngine } from './workflow';
import { ToolRegistry } from './registry';
import { registerBuiltinTools } from './tools';
import { executeAgentLoop } from './executor';
import { sendAgentChatMessage } from '@/lib/ollama';
```

### Vitest Config fÃ¼r E2E
- Separates Test-Target: `vitest.e2e.config.ts`
- Timeout erhÃ¶hen: `testTimeout: 30_000`
- Nicht im normalen `npm run test` (zu langsam)
- Separat: `npm run test:e2e`

---

> **Erstellt von:** ðŸ§ª Test Agent (Sprint 5, 2026-02-18)
> **Referenz:** ADR-001-workflow-engine.md, workflow.ts, workflowTypes.ts
