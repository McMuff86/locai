# LocAI - AI Agent Documentation

> Last Updated: 2025-12-06
> Status: Active Development (Resumed)

---

## Project Overview

**LocAI** is a modern local AI chat application that runs AI models directly on local hardware using Ollama. The project emphasizes privacy, data control, and cloud-independence.

### Key Features
- ğŸ’¬ Local chat with multiple AI models (Llama3, Gemma, Mistral, DeepSeek)
- ğŸ–¼ï¸ Image analysis with vision models
- ğŸ’¾ Local data storage (LocalStorage/FileSystem) with Auto-Save
- ğŸ¨ Dark/Light theme support (Grok-style dark theme)
- ğŸ“± Responsive design with resizable sidebar
- ğŸ” Chat search across conversations
- ğŸ“Š Conversation statistics
- ğŸ¨ **ComfyUI Integration** - Launch & monitor from LocAI

---

## Tech Stack

| Technology | Version | Status |
|------------|---------|--------|
| Next.js | 15.5.7 | âœ… Current (Security patched) |
| React | 19.2.1 | âœ… Current |
| TypeScript | 5.9.3 | âœ… Current |
| Tailwind CSS | 4.1.17 | âœ… Current |
| Framer Motion | 12.23.25 | âœ… Current |
| react-markdown | 10.x | âœ… NEW - GFM support |
| react-syntax-highlighter | 15.x | âœ… NEW - Code highlighting |
| Shadcn/UI | - | âœ… Current |
| Supabase CLI | 2.65.6 | âœ… Current |

---

## Current Architecture

```
src/
â”œâ”€â”€ app/                    # Next.js App Router
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ comfyui/       # ComfyUI Integration APIs
â”‚   â”‚   â”‚   â”œâ”€â”€ launch/route.ts   # Start ComfyUI
â”‚   â”‚   â”‚   â””â”€â”€ status/route.ts   # Check if running
â”‚   â”‚   â””â”€â”€ system-stats/route.ts # System monitoring
â”‚   â”œâ”€â”€ chat/              
â”‚   â”‚   â””â”€â”€ page.tsx       # Chat page (~550 lines, resizable sidebar)
â”‚   â”œâ”€â”€ layout.tsx         
â”‚   â””â”€â”€ globals.css        # Grok/Ollama-style dark theme
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ chat/              # Chat-specific components
â”‚   â”‚   â”œâ”€â”€ ChatContainer.tsx
â”‚   â”‚   â”œâ”€â”€ ChatHeader.tsx
â”‚   â”‚   â”œâ”€â”€ ChatInput.tsx
â”‚   â”‚   â”œâ”€â”€ ChatMessage.tsx    # Uses MarkdownRenderer
â”‚   â”‚   â”œâ”€â”€ ChatSearch.tsx     # Conversation search
â”‚   â”‚   â”œâ”€â”€ ConversationSidebar.tsx  # Settings + ComfyUI widget
â”‚   â”‚   â”œâ”€â”€ ConversationStats.tsx    # Per-chat statistics
â”‚   â”‚   â”œâ”€â”€ MarkdownRenderer.tsx     # GFM + syntax highlighting
â”‚   â”‚   â”œâ”€â”€ SetupCard.tsx
â”‚   â”‚   â”œâ”€â”€ ThinkingProcess.tsx
â”‚   â”‚   â””â”€â”€ TokenCounter.tsx
â”‚   â”œâ”€â”€ ui/                # Shadcn UI components
â”‚   â”œâ”€â”€ ComfyUIWidget.tsx  # NEW: ComfyUI status & launcher
â”‚   â”œâ”€â”€ SystemMonitor.tsx
â”‚   â””â”€â”€ ThemeProvider.tsx
â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ index.ts
â”‚   â”œâ”€â”€ useChat.ts         # Chat + streaming + tokens
â”‚   â”œâ”€â”€ useConversations.ts # Auto-save conversations
â”‚   â”œâ”€â”€ useModels.ts       # Ollama models
â”‚   â”œâ”€â”€ useKeyboardShortcuts.ts
â”‚   â””â”€â”€ useSettings.ts     # NEW: App settings (localStorage)
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ ollama.ts
â”‚   â”œâ”€â”€ storage.ts
â”‚   â”œâ”€â”€ templates/         # Model prompts
â”‚   â””â”€â”€ utils.ts
â””â”€â”€ types/
    â””â”€â”€ chat.ts
```

---

## Pending Updates (npm outdated)

### âœ… Completed Updates (2025-12-06)
All safe updates have been applied:
- React 19.0.0 â†’ 19.2.1
- Framer Motion 12.4.10 â†’ 12.23.25
- Tailwind CSS 4.0.12 â†’ 4.1.17
- TypeScript 5.8.2 â†’ 5.9.3
- All @radix-ui/* packages â†’ latest
- Next.js 15.2.1 â†’ 15.5.7 (Security fix)
- Supabase CLI 2.15.8 â†’ 2.65.6

### ğŸŸ¡ Available Major Updates (Optional)
These are major version updates that may have breaking changes:

| Package | Current | Target | Notes |
|---------|---------|--------|-------|
| next | 15.5.7 | 16.0.7 | Major version - new features |
| uuid | 11.1.0 | 13.0.0 | Major version jump |
| lucide-react | 0.479.0 | 0.556.0 | Icon updates |
| @types/node | 20.19.25 | 24.10.1 | Node.js type definitions |

---

## Upgrade Recommendations

### Phase 1: Quick Wins (Einfachste Upgrades)
1. **Safe dependency updates**: `npm update`
2. **TypeScript strict mode** improvements
3. **Code cleanup** in chat/page.tsx (currently 965 lines)

### Phase 2: Feature Enhancements
1. **Streaming responses** - Enable `stream: true` in Ollama API
2. **Better error handling** with toast notifications
3. **Loading states** optimization

### Phase 3: Architecture Improvements
1. **Supabase integration** - config exists but not implemented
2. **Server Actions** for better security
3. **Component refactoring** - break down large components

---

## Known Issues

1. ~~**Large component**: `src/app/chat/page.tsx` is 965 lines~~ âœ… FIXED (now ~300 lines)
2. ~~**No streaming**: Chat responses are not streamed~~ âœ… FIXED (streaming implemented)
3. **Supabase unused**: Config exists but no database integration
4. **LocalStorage only**: Data persistence is browser-local only

---

## Related Documentation

- [README.md](./README.md) - Project overview and setup
- [README_AIAGENT.MD](./README_AIAGENT.MD) - AI Agent instructions
- [folder_structure.md](./folder_structure.md) - Project structure
- [thoughtprocess/](./thoughtprocess/) - Development thought process

---

## Development Commands

```bash
# Start development server
npm run dev

# Build for production
npm build

# Update safe dependencies
npm update

# Check outdated packages
npm outdated

# Start Ollama (required)
ollama serve

# Pull recommended models
ollama pull llama3
ollama pull llama3.2-vision
```

---

## Changelog

### 2025-12-06
- âœ… Resumed development after pause
- âœ… Created Agents.md documentation
- âœ… Updated all safe dependencies via `npm update`
- âœ… Applied critical security patch (Next.js 15.2.1 â†’ 15.5.7)
  - Fixed: Information exposure in dev server
  - Fixed: Authorization bypass in middleware
  - Fixed: RCE vulnerability in React flight protocol
  - Fixed: SSRF vulnerability
- âœ… 0 vulnerabilities remaining
- âœ… Fixed tsconfig.json to exclude `thoughtprocess/` from compilation
- âœ… Build successful with Next.js 15.5.7
- âœ… Optimized all model templates:
  - DeepSeek R1: Added `<think>` reasoning support
  - Granite Vision: New template for IBM models
  - Llama3 Vision: Enhanced system prompt
  - Llama3/Gemma/Mistral: Improved prompts
- âœ… New Dark Theme (Grok/Ollama style)
  - Deep black background (#141414)
  - Teal/Cyan accent color
  - Enhanced contrast
- âœ… System Monitor Component
  - Real-time CPU usage
  - RAM usage tracking
  - VRAM monitoring via Ollama API
  - Live updates during generation
- âœ… Major Refactoring (968 â†’ ~300 lines in page.tsx)
  - Extracted: useModels, useConversations, useChat hooks
  - Extracted: ChatHeader, SetupCard, TokenCounter components
  - ollama.ts now returns token statistics
  - Token Counter shows: input/output tokens, speed, duration
- âœ… New Features implemented:
  - **Streaming Responses**: Live token-by-token output
  - **Context Window Display**: Shows model's context limit & usage
  - **Keyboard Shortcuts**: Ctrl+N (new), Ctrl+S (save), Escape (stop), / (focus)
  - **Code Block Copy Button**: Click to copy code snippets
  - **Stop Button**: Cancel generation mid-stream
  - **Multi-line Input**: Textarea with Enter/Ctrl+Enter support
- âœ… **Chat Search** (NEW):
  - Full-text search across all conversations
  - Highlights matching text
  - Shows context preview
  - Relevance-based sorting
- âœ… **Markdown Rendering** (NEW):
  - react-markdown with GitHub Flavored Markdown (GFM)
  - Syntax highlighting for 100+ languages via Prism
  - Tables, task lists, strikethrough support
  - Beautiful blockquotes and inline code
  - Copy button for all code blocks
- âœ… **Conversation Statistics** (NEW):
  - Word & character count per conversation
  - Message breakdown (user/assistant)
  - Estimated token count
  - Duration tracking
  - Model information display
  - Expandable stats panel in sidebar

### 2025-03-08 (Last Active)
- Initial project structure
- Basic chat functionality
- Vision model support

